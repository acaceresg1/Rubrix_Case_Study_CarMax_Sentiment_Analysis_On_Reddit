{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1547687f",
   "metadata": {
    "id": "1547687f"
   },
   "source": [
    "## Reddit Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d37cf",
   "metadata": {
    "id": "df3d37cf"
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef14f6",
   "metadata": {
    "id": "39ef14f6"
   },
   "outputs": [],
   "source": [
    "# %pip install praw\n",
    "# %pip install psaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a50186",
   "metadata": {
    "id": "90a50186"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "from prawcore.exceptions import Forbidden\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import calendar\n",
    "import requests\n",
    "import time \n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0aaba7",
   "metadata": {
    "id": "8f0aaba7"
   },
   "source": [
    "### Set Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a78227a",
   "metadata": {
    "id": "5a78227a"
   },
   "outputs": [],
   "source": [
    "# set subreddits and keywords\n",
    "search_dict = {\n",
    "    \"subreddits\": ['SOME_SUBREDDIT'], # Enter the Subreddit(s) you want to scrape\n",
    "    \"keywords\": ['SOME_KEYWORD'] # Enter the Keywords(s) you want to scrape reddit for\n",
    "}\n",
    "\n",
    "years = [2022, 2021, 2020, 2019, 2018, 2017, 2016, 2015] # Enter the years you want to scrape reddit for\n",
    "months = [12,11,10,9,8,7,6,5,4,3,2,1] \n",
    "\n",
    "submission_fields = 'id,score,full_link,subreddit,title,selftext,created_utc,author,num_comments' # chose the data you need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d707a2",
   "metadata": {
    "id": "02d707a2"
   },
   "source": [
    "### Set your praw API credentials \n",
    "##### Check this article on how to set it up:\n",
    "https://towardsdatascience.com/scraping-reddit-data-1c0af3040768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71acf0b",
   "metadata": {
    "id": "d71acf0b"
   },
   "outputs": [],
   "source": [
    "# load Reddit authentication for PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"your_client_id\",           # client id\n",
    "    client_secret=\"your_client_secret\",   # client secret\n",
    "    user_agent=\"your_user_agent\"          # user agent\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc6fff5",
   "metadata": {
    "id": "5bc6fff5"
   },
   "source": [
    "### Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75904c",
   "metadata": {
    "id": "fe75904c"
   },
   "source": [
    "#### Exporting scraped data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a17c6e",
   "metadata": {
    "id": "87a17c6e"
   },
   "outputs": [],
   "source": [
    "def export_to_csv(comment_or_post, word, comments, year, month):\n",
    "    \n",
    "    exported_file_name = f'scraped_reddit_{comment_or_post}_for_{word}_in_{month}_{year}'\n",
    "\n",
    "    comments.to_csv(f'{exported_file_name}.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3173a",
   "metadata": {
    "id": "03b3173a"
   },
   "source": [
    "####  Timestamps for pushshift API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae764a",
   "metadata": {
    "id": "c4ae764a"
   },
   "outputs": [],
   "source": [
    "def before_after_timestamps(year,month):\n",
    "    \n",
    "    if year == dt.datetime.now().year and month == (dt.datetime.now().month):\n",
    "        before = int(time.time())\n",
    "        after = int(dt.datetime.strptime(f'01/{month}/2022 00:00:00', '%d/%m/%Y %H:%M:%S').timestamp())        \n",
    "    else:\n",
    "        before = int(dt.datetime.strptime(f'{calendar.monthrange(year, month)[1]}/{month}/{year} 23:59:59', '%d/%m/%Y %H:%M:%S').timestamp()) \n",
    "        after = int(dt.datetime.strptime(f'01/{month}/{year} 00:00:00', '%d/%m/%Y %H:%M:%S').timestamp()) \n",
    "    \n",
    "    return before, after\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b8d2a",
   "metadata": {
    "id": "ba1b8d2a"
   },
   "source": [
    "#### Text formatting of scraped texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ab139",
   "metadata": {
    "id": "9f6ab139"
   },
   "outputs": [],
   "source": [
    "# function that formats text for readability \n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.strip()\n",
    "    text = re.sub('\\n+', '\\n', text)\n",
    "    text = re.sub('&amp;', '&', text)\n",
    "    text = re.sub('&lt;', '<', text)\n",
    "    text = re.sub('&gt;', '>', text)\n",
    "    text = re.sub('&#x200B;', '', text)\n",
    "    text = re.sub('&nbsp;', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed60e8d6",
   "metadata": {
    "id": "ed60e8d6"
   },
   "outputs": [],
   "source": [
    "def clean_and_format_dataframe(df):\n",
    "\n",
    "    df['body'] = df['body'].apply(lambda text: clean_text(text))\n",
    "    df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "    df['date'] = df['created_utc'].apply(lambda x: pd.Timestamp.to_pydatetime(x))\n",
    "    df['link'] = 'https://www.reddit.com' + df['permalink']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc084449",
   "metadata": {
    "id": "fc084449"
   },
   "source": [
    "#### Scraping the comments for each post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93975d",
   "metadata": {
    "id": "9e93975d"
   },
   "outputs": [],
   "source": [
    "def scrape_comments_of_posts(posts):\n",
    "        \n",
    "    # search Reddit comments  using Pushshift\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    error_count = 0\n",
    "    \n",
    "    for index, post in posts.iterrows():\n",
    "        \n",
    "        print(f'Scraping comments for post {index + 1} of {len(posts.id)}', end='\\x1b[1K\\r')\n",
    "        \n",
    "        submission = reddit.submission(id=post.id)\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        \n",
    "        for comment in submission.comments.list():\n",
    "            try:\n",
    "                row = [\n",
    "                    comment.parent_id, \n",
    "                    comment.id, \n",
    "                    comment.score, \n",
    "                    comment.created, \n",
    "                    comment.body, \n",
    "                    comment.score, \n",
    "                    comment.permalink,\n",
    "                    comment.is_submitter,\n",
    "                    comment.author\n",
    "                ]\n",
    "                data.append(row)\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                continue\n",
    "    \n",
    "    data = [x for x in data if x != []] # delete empty lists in data\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=[\n",
    "        'parent_id', \n",
    "        'comment_id', \n",
    "        'score_id', \n",
    "        'created_utc', \n",
    "        'body','score', \n",
    "        'permalink', \n",
    "        'is_submitter',\n",
    "        'author'\n",
    "    ])\n",
    "    \n",
    "    df = clean_and_format_dataframe(df)\n",
    "    \n",
    "    print(f'total_posts_found {len(data)}')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96633000",
   "metadata": {
    "id": "96633000"
   },
   "source": [
    "#### Correcting data of each post with Praw API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df18f9",
   "metadata": {
    "id": "41df18f9"
   },
   "outputs": [],
   "source": [
    "def get_data_from_praw(data):\n",
    "    \n",
    "    data = data\n",
    "\n",
    "    for count, d in enumerate(data):\n",
    "        try:\n",
    "            print(f\"Progress: {count+1} / {len(data)}\", end='\\x1b[1K\\r')\n",
    "            # get data from PRAW based on unique post ID from Pushshift\n",
    "            submission = reddit.submission(id=d['id'])\n",
    "            submission.comment_sort = 'top'\n",
    "\n",
    "            d.update({'score': submission.score})\n",
    "            #d.update({'post keywords': keywords}) # for reference in csv\n",
    "            d.update({'date': dt.datetime.fromtimestamp(d['created_utc']).date()})\n",
    "            try:\n",
    "                d.update({'comment_score': submission.comments[0].score})\n",
    "                d.update({'top_comment': clean_text(submission.comments[0].body)})\n",
    "            except:\n",
    "                d.update({'comment_score': \"N/A\"})\n",
    "                d.update({'top_comment': \"N/A\"})\n",
    "            d.update({'title': clean_text(d.get(\"title\",\"N/A\"))})\n",
    "            d.update({'selftext': clean_text(d.get(\"selftext\",\"N/A\"))})\n",
    "\n",
    "            column_order = ['full_link', 'subreddit', 'post keywords', 'id', 'date', 'score', 'num_comments', 'author', 'title', 'selftext', 'top_comment', 'comment_score']\n",
    "            df = pd.DataFrame(data, columns=column_order).drop_duplicates()\n",
    "        except Forbidden:\n",
    "            continue\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc45b93a",
   "metadata": {
    "id": "bc45b93a"
   },
   "source": [
    "#### Scraping Pushshift API for posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98ab72",
   "metadata": {
    "id": "cc98ab72"
   },
   "outputs": [],
   "source": [
    "def scrape_reddit_posts(keyword_or_subreddit, word, year, month):\n",
    "    \n",
    "    if keyword_or_subreddit == 'keywords':\n",
    "        search = f\"q={word}\"  \n",
    "    elif keyword_or_subreddit == 'subreddits':\n",
    "        search = f\"subreddit={word}\"\n",
    "    \n",
    "    before, after = before_after_timestamps(year, month)\n",
    "    \n",
    "    url = f\"https://api.pushshift.io/reddit/search/submission/?{search}&fields={submission_fields}&after={after}&size=1000&sort=desc&metadata=true\"\n",
    "\n",
    "    # search Reddit submissions (posts) using Pushshift\n",
    "    start_from = '&before=' + str(before)\n",
    "    first_pass = True\n",
    "    data = []\n",
    "\n",
    "    total_posts_found = 0 \n",
    "    error_count = 0\n",
    "\n",
    "    while True:\n",
    "        if first_pass: \n",
    "            print(f\"Collecting Reddit data for {word} in {month}/{year}...\\n\")\n",
    "            try:\n",
    "                request = requests.get(url+start_from)\n",
    "                posts = request.json()\n",
    "                \n",
    "                first_pass = False\n",
    "            except ValueError:\n",
    "                error_count += 1\n",
    "                first_pass = True\n",
    "                continue\n",
    "\n",
    "            difference = posts['metadata']['shards'][\"successful\"] - posts['metadata']['shards'][\"total\"]\n",
    "            total_posts_found = posts['metadata']['total_results']\n",
    "            \n",
    "            print(f\"{posts['metadata']['total_results']} {word}-posts found\")\n",
    "            \n",
    "            if abs(difference) > 0:\n",
    "                print(f\"Warning {abs(difference)} shards are missing.\")\n",
    "        else:\n",
    "            try:\n",
    "                request = requests.get(url+start_from)\n",
    "                posts = request.json()\n",
    "            except ValueError:\n",
    "                error_count += 1\n",
    "                continue\n",
    "        \n",
    "        if abs(difference) > 0:\n",
    "            print(f\"JSONDecodeError count: {error_count}\", end=\"\\r\")\n",
    "        \n",
    "        print(f\"Progress: {len(data)} / {total_posts_found} Remaining: {posts['metadata']['total_results']}\", end='\\x1b[1K\\r')\n",
    " \n",
    "        data.extend(posts[\"data\"])\n",
    "        if len(posts[\"data\"]) == 0:\n",
    "            break # stop collecting data once there's nothing left to collect\n",
    "\n",
    "        last_utc = data[-1]['created_utc']\n",
    "        start_from = '&before=' + str(last_utc)\n",
    "    \n",
    "    if abs(difference) > 0:\n",
    "        print(f\"\\r\\nsuccessful data collection!\\n{len(data)} of {total_posts_found} total collected. Missing posts due to missing shards\")\n",
    "    else:\n",
    "        print(f\"\\r\\nsuccessful data collection!\\n{len(data)} of {total_posts_found} total collected.\")\n",
    "    \n",
    "    # updating and completeing post data with differnt PRAW API\n",
    "    print(f\"\\nUpdating and completeing data of {word}-posts with PRAW API. Approx 30 min/1000 posts\")\n",
    "    df = get_data_from_praw(data)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556980c",
   "metadata": {
    "id": "7556980c"
   },
   "source": [
    "#### Putting together functions from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8429969",
   "metadata": {
    "id": "c8429969"
   },
   "outputs": [],
   "source": [
    "def scrape_reddit(keyword_or_subreddit, word, year, month):\n",
    "    \n",
    "    # scraping posts\n",
    "    posts = scrape_reddit_posts(keyword_or_subreddit,word, year, month)\n",
    "    export_to_csv('post', word, posts, year, month)\n",
    "        \n",
    "    # scraping comments\n",
    "    comments = scrape_comments_of_posts(posts)\n",
    "    export_to_csv('comments', word, comments, year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80faa4b",
   "metadata": {
    "id": "c80faa4b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  \n",
    "    for key in search_dict:\n",
    "        for year in years:\n",
    "            for month in months:\n",
    "                if year == dt.datetime.now().year and month > dt.datetime.now().month:\n",
    "                    continue\n",
    "                else:\n",
    "                    for word in search_dict[key]:\n",
    "                        scrape_reddit(key, word, year, month)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250f371",
   "metadata": {
    "id": "4250f371",
    "outputId": "efa7239c-2f86-4bb2-de80-f1cf531a49b0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Reddit data for whatcarshouldIbuy in 5/2022...\n",
      "\n",
      "3729 whatcarshouldIbuy-posts found\n",
      "Progress: 3729 / 3729 Remaining: 0\u001b[1KKKK\n",
      "successful data collection!\n",
      "3729 of 3729 total collected.\n",
      "\n",
      "Updating and completeing data of whatcarshouldIbuy-posts with PRAW API. Approx 30 min/1000 posts\n",
      "total_posts_found 36689ost 3729 of 3729\u001b[1K\n",
      "Collecting Reddit data for whatcarshouldIbuy in 4/2022...\n",
      "\n",
      "3417 whatcarshouldIbuy-posts found\n",
      "Progress: 3415 / 3417 Remaining: 0\u001b[1KKKK\n",
      "successful data collection!\n",
      "3415 of 3417 total collected.\n",
      "\n",
      "Updating and completeing data of whatcarshouldIbuy-posts with PRAW API. Approx 30 min/1000 posts\n",
      "total_posts_found 34700ost 3415 of 3415\u001b[1K\n",
      "Collecting Reddit data for whatcarshouldIbuy in 3/2022...\n",
      "\n",
      "3135 whatcarshouldIbuy-posts found\n",
      "Progress: 3134 / 3135 Remaining: 0\u001b[1KKKK\n",
      "successful data collection!\n",
      "3134 of 3135 total collected.\n",
      "\n",
      "Updating and completeing data of whatcarshouldIbuy-posts with PRAW API. Approx 30 min/1000 posts\n",
      "total_posts_found 30761ost 3134 of 3134\u001b[1K\n",
      "Collecting Reddit data for whatcarshouldIbuy in 2/2022...\n",
      "\n",
      "2898 whatcarshouldIbuy-posts found\n",
      "Progress: 2898 / 2898 Remaining: 0\u001b[1KKKK\n",
      "successful data collection!\n",
      "2898 of 2898 total collected.\n",
      "\n",
      "Updating and completeing data of whatcarshouldIbuy-posts with PRAW API. Approx 30 min/1000 posts\n",
      "total_posts_found 30809ost 2898 of 2898\u001b[1K\n",
      "Collecting Reddit data for whatcarshouldIbuy in 1/2022...\n",
      "\n",
      "2963 whatcarshouldIbuy-posts found\n",
      "Progress: 2962 / 2963 Remaining: 0\u001b[1KKKK\n",
      "successful data collection!\n",
      "2962 of 2963 total collected.\n",
      "\n",
      "Updating and completeing data of whatcarshouldIbuy-posts with PRAW API. Approx 30 min/1000 posts\n",
      "total_posts_found 28917ost 2962 of 2962\u001b[1K\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Capstone_Reddit_Scraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
